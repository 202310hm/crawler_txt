#robots.txt や利用規約を再度確認し、倫理的な配慮を忘れずに行ってください。
#urls.txtにスクレイピングしたいurlを１行に１つづつ書く。
#crawler.pyにurlをrun。テキストデータ抽出。
#unique_txt.pyで重複する文字を削除したデータを抽出。